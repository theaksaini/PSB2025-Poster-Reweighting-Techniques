{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686427c1-5057-47e0-ba13-94a550f337fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "# PARETO FRONT TOOLS\n",
    "def check_dominance(p1,p2):\n",
    "    \"\"\"check if p1 dominates p2. Assuming that the objectives are to be minimized.\"\"\"\n",
    "    flag1 = 0\n",
    "    flag2 = 0\n",
    "\n",
    "    for o1,o2 in zip(p1,p2):\n",
    "        if o1 < o2:\n",
    "            flag1 = 1\n",
    "        elif o1 > o2:\n",
    "            flag2 = 1\n",
    "\n",
    "    if flag1==1 and flag2 == 0:\n",
    "        return 1\n",
    "    elif flag1==0 and flag2 == 1:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def front(obj1,obj2):\n",
    "    \"\"\"return indices from x and y that are on the Pareto front. Assume that the objectives are to be minimized.\"\"\"\n",
    "    rank = []\n",
    "    assert(len(obj1)==len(obj2))\n",
    "    n_inds = len(obj1)\n",
    "    front = []\n",
    "\n",
    "    for i in np.arange(n_inds):\n",
    "        p = (obj1[i],obj2[i])\n",
    "        dcount = 0\n",
    "        dom = []\n",
    "        for j in np.arange(n_inds):\n",
    "            q = (obj1[j],obj2[j])\n",
    "            compare = check_dominance(p,q)\n",
    "            if compare == 1:\n",
    "                dom.append(j)\n",
    "            elif compare == -1:\n",
    "                dcount = dcount +1\n",
    "\n",
    "        if dcount == 0:\n",
    "            front.append(i)\n",
    "\n",
    "    f_obj2 = [obj2[f] for f in front]\n",
    "    s2 = np.argsort(np.array(f_obj2))\n",
    "    front = [front[s] for s in s2]\n",
    "\n",
    "    return front\n",
    "\n",
    "print(front([1, 2, 3, 0, 4], [0, 1, 2, 1, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde7cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypervolume of pareto front for different datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deap.tools._hypervolume import pyhv\n",
    "\n",
    "# Need to optimize this code\n",
    "def save_hv_results(base_result_folder, hv_folder, taskid, experiments, objectives, runs):\n",
    "    assert os.path.isdir(hv_folder), f\"Folder to save HV values does not exist: {hv_folder}\"\n",
    "\n",
    "    hv_df = pd.DataFrame(columns = ['hv', 'dataset', 'exp', 'num_pts', 'avg_pred_perf', 'avg_fair_perf']) # results on training data\n",
    "    hv_test_df = pd.DataFrame(columns = ['hv', 'dataset', 'exp', 'num_pts', 'avg_pred_perf', 'avg_fair_perf']) # results on test data\n",
    "\n",
    "    for exp in experiments:\n",
    "        print(\"Processing experiment:\", exp)\n",
    "        for rep in range(runs):\n",
    "            print(\"Processing run:\", rep)\n",
    "        \n",
    "            save_folder = f\"{base_result_folder}/{taskid}_{rep}_{exp}\"\n",
    "            # If hv_values file exist, take the values from there.\n",
    "            if os.path.exists(f\"{save_folder}/hv_values.pkl\"):\n",
    "                with open(f\"{save_folder}/hv_values.pkl\",'rb') as file:\n",
    "                    hv_file = pd.read_pickle(file)\n",
    "                hv_df.loc[len(hv_df.index)] = {'hv' : hv_file['train_hv']['hv'], 'dataset' : taskid, 'exp' : exp, 'num_pts':hv_file['train_hv']['num_pts'],'avg_pred_perf':hv_file['train_hv']['avg_pred_perf'], 'avg_fair_perf':hv_file['train_hv']['avg_fair_perf']}\n",
    "                hv_test_df.loc[len(hv_test_df.index)] = {'hv' : hv_file['test_hv']['hv'], 'dataset' : taskid, 'exp' : exp, 'num_pts':hv_file['test_hv']['num_pts'],'avg_pred_perf':hv_file['test_hv']['avg_pred_perf'], 'avg_fair_perf':hv_file['test_hv']['avg_fair_perf']}\n",
    "                \n",
    "            else:\n",
    "                x_vals = [] # auroc, etc\n",
    "                y_vals = [] # fnr, etc.\n",
    "\n",
    "                results_file = f\"{save_folder}/scores.pkl\"\n",
    "                with open(results_file,'rb') as file:\n",
    "                    this_df = pd.read_pickle(file)\n",
    "        \n",
    "                # Each run has 5000 individuals, and therefore will have one hv for validation set and one for test set\n",
    "                # based on pareto-front derived from these 1000 individuals\n",
    "                train_pred_perf = this_df['train_'+objectives[0]].to_numpy()\n",
    "                x_vals = 1-train_pred_perf\n",
    "                y_vals = this_df['train_'+objectives[1]].to_numpy()\n",
    "\n",
    "                PF = front(x_vals,y_vals)\n",
    "                pf_x = [x_vals[i] for i in PF]\n",
    "                pf_y = [y_vals[i] for i in PF]\n",
    "                hv = pyhv.hypervolume([(xi,yi) for xi,yi in zip(pf_x,pf_y)], ref=np.array([1,1]))\n",
    "                avg_pred_perf = np.mean([1-x for x in pf_x])\n",
    "                avg_fair_perf = np.mean(pf_y)\n",
    "                hv_df.loc[len(hv_df.index)] = {'hv' : hv, 'dataset' : task_id, 'exp' : exp, 'num_pts':len(pf_x),'avg_pred_perf':avg_pred_perf, 'avg_fair_perf':avg_fair_perf}\n",
    "        \n",
    "                test_pred_perf = this_df[objectives[0]].to_numpy()\n",
    "                x_vals = 1-test_pred_perf\n",
    "                y_vals = this_df[objectives[1]].to_numpy()\n",
    "\n",
    "                PF = front(x_vals,y_vals)\n",
    "                pf_x = [x_vals[i] for i in PF]\n",
    "                pf_y = [y_vals[i] for i in PF]\n",
    "                hv = pyhv.hypervolume([(xi,yi) for xi,yi in zip(pf_x,pf_y)], ref=np.array([1,1]))\n",
    "                avg_pred_perf = np.mean([1-x for x in pf_x])\n",
    "                avg_fair_perf = np.mean(pf_y)\n",
    "                hv_test_df.loc[len(hv_test_df.index)] = {'hv' : hv, 'dataset' : task_id, 'exp' : exp, 'num_pts':len(pf_x),'avg_pred_perf':avg_pred_perf, 'avg_fair_perf':avg_fair_perf}\n",
    "\n",
    "    hv_train_file = f\"{hv_folder}/hv_train_{task_id}.csv\"\n",
    "    hv_test_file = f\"{hv_folder}/hv_test_{task_id}.csv\"\n",
    "    hv_df.to_csv(hv_train_file)\n",
    "    hv_test_df.to_csv(hv_test_file)\n",
    "\n",
    "task_ids = ['heart_disease', 'student_math', 'us_crime', 'nlsy', 'compas', 'law_school','pmad_phq', 'pmad_epds']\n",
    "experiments1 = ['Equal Weights',\n",
    "               'Deterministic Weights',\n",
    "              'Evolved Weights']\n",
    "objective_functions=['accuracy', 'subgroup_FNR_loss']\n",
    "#objective_functions=['accuracy', 'demographic_parity_difference']\n",
    "results_folder = '~/Documents/Results/results10'\n",
    "hv_folder = '~/Documents/Results/hv_values10'\n",
    "\n",
    "files_dir = [\n",
    "    f for f in os.listdir(results_folder) if os.path.isdir(os.path.join(results_folder, f))\n",
    "]\n",
    "print(files_dir)\n",
    "\n",
    "for task_id in task_ids:\n",
    "    print('Processing task_id:', task_id)\n",
    "    save_hv_results(f'{results_folder}/{files_dir[0]}', hv_folder, task_id, experiments1,objective_functions, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "341014e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_folder = '/Users/sainia3/Documents/Results/hv_values10'\n",
    "\n",
    "# List of CSV files to be merged\n",
    "csv_files = [f\"{hv_folder}/hv_test_{task_id}.csv\" for task_id in task_ids]\n",
    "\n",
    "# Read and concatenate all CSV files into one dataframe\n",
    "df_list = [pd.read_csv(file) for file in csv_files]  # Reading each file into a DataFrame\n",
    "merged_df = pd.concat(df_list, ignore_index=True)    # Concatenating DataFrames\n",
    "\n",
    "# Save the merged dataframe into a new CSV file\n",
    "output_file = f\"{hv_folder}/hv_test.csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Merged CSV file saved as {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
